---
layout: post
title: MARL
published: false
---

An overview of the paper “[Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms](https://arxiv.org/pdf/1911.10635.pdf)”.
<!--break-->
The authors propose a general overview of research in Multi-Agent reiforcement learning. All images and tables in this post are from their paper.

## Introduction

MARl addresses the sequential decision-making problem of multiple autonomous agents that operate in a common environment, each of which aims to optimize its own long-term return by interacting with the environment and other agents. Algorithms in general can be placed into different groups:
* **Cooperative:** model collaborate to optimize a common long-term goal.
* **Competitive:** Zero-sum games
* **Mix:** General-sum returns

There are several challenges in MARL across different settings such as:
* Multidimensional goals leading to challenge in dealing with equilibrium points.
* Additional performance criteria beyond optimization, such as the efficiency of communication/coordination and robustness against potential adversarial agents.
* Concurrent policy improvement of "egoist" agents makes the environment non-stationary.
* The joint action space increases exponentially with the number of agents and may cause scalability issues.
* Information structure, such as how much information each agent has access to.

## Background

### Single-Agent RL

A RL agent is modeled to perform sequential decision-making by interacting with the environment usually defined as a MDP. A *Markov decision process* is defined by a tuple <img src="https://latex.codecogs.com/svg.latex?(S,A,P,R,\gamma)" title="(S,A,P,R,\gamma)" /> where:
* <img src="https://latex.codecogs.com/svg.latex?S" title="S" /> is state space, <img src="https://latex.codecogs.com/svg.latex?A" title="A" /> is action space.
* <img src="https://latex.codecogs.com/svg.latex?P:S \times A \rightarrow \bigtriangleup (S)" title="P:S \times A \rightarrow \bigtriangleup (S)" /> is the transition probability of any state given action.
* <img src="https://latex.codecogs.com/svg.latex?R" title="R" /> is the immediate reward received after the transition.
* <img src="https://latex.codecogs.com/svg.latex?\gamma" title="\gamma" /> is the discount factor.

MDP is used to formalize the decision-making with full observability of state <img src="https://latex.codecogs.com/svg.latex?s" title="s" />. The goal of MDP is to find a policy such that the discounted accumulated reward is maximized.

### Multi-Agent RL

Both the evolution of the system state and the reward received by each agent influenced by the joint actions of all agents. Each agent now has its own long-term reward to optimize, which now becomes a function of the policies of all other agents. There are predominantly two theoretical frameworks for MARL:
* Markov/Stochastic games
* Extensive form games

<p align="center">
<b> Schematic diagrams of the system evolution of a MDPs, Markov game and Extensive-form game.</b>
</p>
<p align="center">
<img src="/assets/Papers/2/Figure-6.png?raw=true" alt="Figure 1"/>
</p>

#### Markov/Stochastic Game

The Markov game is simply the extension of the MDPs to all the agents in the environment. The tuple is now simply defined as: <img src="https://latex.codecogs.com/svg.latex?(S,\{A^i\}_N,P,\{R^i\}_N,\gamma)" title="(S,\{A^i\}_N,P,\{R^i\}_N,\gamma)" />.
To see how it works, assume at time <img src="https://latex.codecogs.com/svg.latex?t" title="t" />, each agent executes an action <img src="https://latex.codecogs.com/svg.latex?a^i_t" title="a^i_t" /> and transition to <img src="https://latex.codecogs.com/svg.latex?s_{t+1}" title="s_{t+1}" />. The agents take their actions simultaneously for the given state. The goal of the agent is to optimize its own long-term reward, by finding the policy. The goal is to find the Nash equilibrium in MARL. As a standard learning goal for MARL, NE always exists for discounted rewards, but may not be unique in general.

* For fully cooperative setting, all the agents follow a common overall goal. This can also be viewed as a special case of *Markov Potential games* with potential function being the common accumulated reward. This allows the single-agent RL algorithms to be applied, if all agents are coordinated as one decision maker.
* In the (Partially) cooperative setting, we have a team-average reward. Agents are allowed to have a intrinsic reward. Such heterogeneity also necessitates the incorporation of communication protocols into MARL and the analysis of communication-efficient MARL algorithms.
* For fully competitive setting, we typically model as zero-sum Markov games. The scenario is mainly focussed on two agents. This leads to buliding a robust policy.
* In the mixed setting, each agent is self interested, and reward may be conflicting with others. Nash equilibrium is the most significant influence in these settings.

#### Extensive-Form games

Markov games can only handle the fully-observed case. A lot of MARL applications involve agents with only partial observability, i.e., imperfect information of the game. This is much more sophisticated that the previous setting. In this setting, we again have  <img src="https://latex.codecogs.com/svg.latex?N" title="N" /> agents, and we add a *chance of nature agent* <img src="https://latex.codecogs.com/svg.latex?c" title="c" />, which has a fixed stochastic policy that specifies the randomness of the environment. Besides the set of all possible actions an agent can take, and <img src="https://latex.codecogs.com/svg.latex?\mathcal{H}" title="\mathcal{H}" /> is the set of all possible *histories*, where each history is a sequence of actions taken from the beginning of the game. Let <img src="https://latex.codecogs.com/svg.latex?\mathcal{A}(h)" title="\mathcal{A}(h)" /> denote the set of actions available after a nonterminal history <img src="https://latex.codecogs.com/svg.latex?h" title="h" />. Upon reaching terminal history, a utility is assigned to each agent called the *identification function* which specifies which agent takes the action at each history.

The assumption of perfect recall is commonly made in the literature, enables the existence of polynomial time algorithms for solving the game.

## Challenges in MARL Theory

The approach proposed by the authors is a centralized version of the policy gradient approach.

<p align="center">
<img src="https://latex.codecogs.com/svg.latex?\bigtriangledown J(\theta_i) = \mathbb{E}[\bigtriangledown_{\theta_i}\log \pi_i(a_i|o_i)Q_i^{\pi}(x,a_1,a_2,...,a_N)]" title="\bigtriangledown J(\theta_i) = \mathbb{E}[\bigtriangledown_{\theta_i}\log \pi_i(a_i|o_i)Q_i^{\pi}(x,a_1,a_2,...,a_N)]" />
</p>

Here <img src="https://latex.codecogs.com/svg.latex?Q" title="Q" /> is a *centralized action-value function* that takes as input the actions of all agents, in addition to some state informaion (concatenation of all states of agents). Since each  is learned separately, agents can have arbitrary reward structures, including conflicting rewards in a competitive setting.

### Inferring Policies of Other Agents

To remove the assumption of knowing other agents' policies, an approximate policy is learned by maximizing the log probability of j's actions with an entropy regularizer.

### Agents with Policy Ensembles

To aid exploration process, they use a ensemble of policies to acoid overfitting and non-stationarity due to the agents' changing policies.

## Conclusion

The authors propose an approach for MARL, and experiment on different environments such as different settings of *multi-agent particle environments*. Another important reference to share for a baseline for MARL includes "[pettingzoo.ml](https://www.pettingzoo.ml/envs)".
